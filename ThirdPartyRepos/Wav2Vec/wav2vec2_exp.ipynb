{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Wav2Vec - English Single"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import numpy as np\n",
    "model_name = \"skpawar1305/wav2vec2-base-finetuned-digits\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "db = -21.15406995820191\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import sounddevice as sd\n",
    "\n",
    "test_file = \"12345_68772.wav\"\n",
    "\n",
    "def get_nparray(audiosegment: AudioSegment):\n",
    "    samples = audiosegment.get_array_of_samples()\n",
    "    samples_float = librosa.util.buf_to_float(samples,n_bytes=4, dtype=np.float32)\n",
    "    if audiosegment.channels==2:\n",
    "        sample_left= np.copy(samples_float[::2])\n",
    "        sample_right= np.copy(samples_float[1::2])\n",
    "        sample_all = np.array([sample_left,sample_right])\n",
    "    else:\n",
    "        sample_all = samples_float\n",
    "\n",
    "    return sample_all\n",
    "\n",
    "\n",
    "data = AudioSegment.from_wav(test_file)\n",
    "print(f\"db = {data.dBFS}\")\n",
    "sd.play(get_nparray(data), samplerate=16000)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:07<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "chunks = split_on_silence(data, min_silence_len=20, keep_silence=40, silence_thresh=-25)\n",
    "print(len(chunks))\n",
    "for _c in tqdm(chunks):\n",
    "    sd.play(get_nparray(_c), samplerate=16000)\n",
    "    sd.wait()\n",
    "\n",
    "# sd.play(get_nparray(data))\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "_unknown_\n",
      "_unknown_\n",
      "two\n",
      "_unknown_\n",
      "_unknown_\n",
      "three\n",
      "four\n",
      "five\n"
     ]
    }
   ],
   "source": [
    "def inference(chunk):\n",
    "    inputs = feature_extractor(chunk, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    labels = [model.config.id2label[_id] for _id in predicted_ids.tolist()]\n",
    "    if not len(labels):\n",
    "        print(\"dead\")\n",
    "    return labels[0]\n",
    "\n",
    "for _c in chunks:\n",
    "    print(inference(get_nparray(_c)))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wav2Vec - Chinese Sequence\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/09/2022 20:18:25 - INFO - huggingsound.speech_recognition.model - Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"wbbbbb/wav2vec2-large-chinese-zh-cn\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 5168,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 5171,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
      "\n",
      "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at wbbbbb/wav2vec2-large-chinese-zh-cn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n",
      "loading configuration file preprocessor_config.json from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\preprocessor_config.json\n",
      "Feature extractor Wav2Vec2FeatureExtractor {\n",
      "  \"do_normalize\": true,\n",
      "  \"feature_extractor_type\": \"Wav2Vec2FeatureExtractor\",\n",
      "  \"feature_size\": 1,\n",
      "  \"padding_side\": \"right\",\n",
      "  \"padding_value\": 0,\n",
      "  \"return_attention_mask\": true,\n",
      "  \"sampling_rate\": 16000\n",
      "}\n",
      "\n",
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading configuration file config.json from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"wbbbbb/wav2vec2-large-chinese-zh-cn\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 5168,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 5171,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading file vocab.json from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\vocab.json\n",
      "loading file tokenizer_config.json from cache at None\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\special_tokens_map.json\n",
      "loading configuration file config.json from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"wbbbbb/wav2vec2-large-chinese-zh-cn\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 5168,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 5171,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "Adding <s> to the vocabulary\n",
      "Adding </s> to the vocabulary\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from huggingsound import SpeechRecognitionModel\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "model = SpeechRecognitionModel(\"wbbbbb/wav2vec2-large-chinese-zh-cn\", device=\"cuda\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "dataset = './zh_seq'\n",
    "dataset = [os.path.join(dataset, f) for f in os.listdir(dataset)]\n",
    "eval_files = dataset.copy()\n",
    "tmp = []\n",
    "for d in dataset:\n",
    "    label = os.path.splitext(os.path.split(d)[1])[0].split('_')[0]\n",
    "    tmp.append({\"path\": d, \"transcription\": label})\n",
    "dataset = tmp\n",
    "del tmp"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 24/24 [00:17<00:00,  1.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'end_timestamps': [980],\n",
      "  'probabilities': [0.8219219446182251],\n",
      "  'start_timestamps': [960],\n",
      "  'transcription': '伊'},\n",
      " {'end_timestamps': [860, 1360, 1800, 2260],\n",
      "  'probabilities': [0.9657955169677734,\n",
      "                    0.9955781102180481,\n",
      "                    0.7799317240715027,\n",
      "                    0.8337723612785339],\n",
      "  'start_timestamps': [840, 1340, 1780, 2240],\n",
      "  'transcription': '一一一一'},\n",
      " {'end_timestamps': [680, 1180, 1840, 2440, 3000],\n",
      "  'probabilities': [0.9978392720222473,\n",
      "                    0.999473512172699,\n",
      "                    0.9999866485595703,\n",
      "                    0.9977414608001709,\n",
      "                    0.9924236536026001],\n",
      "  'start_timestamps': [660, 1160, 1820, 2420, 2980],\n",
      "  'transcription': '一二三四五'},\n",
      " {'end_timestamps': [960],\n",
      "  'probabilities': [0.9705238342285156],\n",
      "  'start_timestamps': [940],\n",
      "  'transcription': '七'},\n",
      " {'end_timestamps': [1120, 1700, 2240],\n",
      "  'probabilities': [0.9857266545295715, 0.9892627596855164, 0.9728983640670776],\n",
      "  'start_timestamps': [1100, 1680, 2220],\n",
      "  'transcription': '七七七'},\n",
      " {'end_timestamps': [1040],\n",
      "  'probabilities': [0.9806867241859436],\n",
      "  'start_timestamps': [1020],\n",
      "  'transcription': '三'},\n",
      " {'end_timestamps': [980, 1500, 2100, 2640],\n",
      "  'probabilities': [0.9999196529388428,\n",
      "                    0.9931943416595459,\n",
      "                    0.9996508359909058,\n",
      "                    0.9981867671012878],\n",
      "  'start_timestamps': [960, 1480, 2080, 2620],\n",
      "  'transcription': '三三三三'},\n",
      " {'end_timestamps': [820],\n",
      "  'probabilities': [0.942906379699707],\n",
      "  'start_timestamps': [800],\n",
      "  'transcription': '九'},\n",
      " {'end_timestamps': [820, 1280, 1780, 2340],\n",
      "  'probabilities': [0.9986823201179504,\n",
      "                    0.999603807926178,\n",
      "                    0.9996546506881714,\n",
      "                    0.9999572038650513],\n",
      "  'start_timestamps': [800, 1260, 1760, 2320],\n",
      "  'transcription': '九九九九'},\n",
      " {'end_timestamps': [960, 1540, 2100, 2680, 3260],\n",
      "  'probabilities': [0.42328405380249023,\n",
      "                    0.6567498445510864,\n",
      "                    0.3632156550884247,\n",
      "                    0.5863621830940247,\n",
      "                    0.8020877838134766],\n",
      "  'start_timestamps': [940, 1520, 2080, 2660, 3240],\n",
      "  'transcription': '教教教敬敬'},\n",
      " {'end_timestamps': [980, 1160],\n",
      "  'probabilities': [0.9931465983390808, 0.9682826399803162],\n",
      "  'start_timestamps': [960, 1140],\n",
      "  'transcription': '阿尔'},\n",
      " {'end_timestamps': [860, 1300, 1840, 2380],\n",
      "  'probabilities': [0.9993278980255127,\n",
      "                    0.9984221458435059,\n",
      "                    0.9997219443321228,\n",
      "                    0.9995243549346924],\n",
      "  'start_timestamps': [840, 1280, 1820, 2360],\n",
      "  'transcription': '二二二二'},\n",
      " {'end_timestamps': [940],\n",
      "  'probabilities': [0.817608118057251],\n",
      "  'start_timestamps': [920],\n",
      "  'transcription': '五'},\n",
      " {'end_timestamps': [660, 1240, 1800, 2360],\n",
      "  'probabilities': [0.4242343008518219,\n",
      "                    0.35080260038375854,\n",
      "                    0.48595118522644043,\n",
      "                    0.25724542140960693],\n",
      "  'start_timestamps': [640, 1220, 1780, 2340],\n",
      "  'transcription': '五无无无'},\n",
      " {'end_timestamps': [820],\n",
      "  'probabilities': [0.980006992816925],\n",
      "  'start_timestamps': [800],\n",
      "  'transcription': '八'},\n",
      " {'end_timestamps': [760, 1220, 1800, 2300],\n",
      "  'probabilities': [0.9986911416053772,\n",
      "                    0.996300458908081,\n",
      "                    0.9972794651985168,\n",
      "                    0.9954042434692383],\n",
      "  'start_timestamps': [740, 1200, 1780, 2280],\n",
      "  'transcription': '八八八八'},\n",
      " {'end_timestamps': [1120],\n",
      "  'probabilities': [0.9997170567512512],\n",
      "  'start_timestamps': [1100],\n",
      "  'transcription': '六'},\n",
      " {'end_timestamps': [800, 1480, 2060, 2620, 3240],\n",
      "  'probabilities': [0.9996196031570435,\n",
      "                    0.9999744892120361,\n",
      "                    0.9999183416366577,\n",
      "                    0.9998632669448853,\n",
      "                    0.9381705522537231],\n",
      "  'start_timestamps': [780, 1460, 2040, 2600, 3220],\n",
      "  'transcription': '六七八九十'},\n",
      " {'end_timestamps': [760, 1300, 1820, 2420],\n",
      "  'probabilities': [0.999919056892395,\n",
      "                    0.9999462366104126,\n",
      "                    0.999923825263977,\n",
      "                    0.9999572038650513],\n",
      "  'start_timestamps': [740, 1280, 1800, 2400],\n",
      "  'transcription': '六六六六'},\n",
      " {'end_timestamps': [1020],\n",
      "  'probabilities': [0.44449105858802795],\n",
      "  'start_timestamps': [1000],\n",
      "  'transcription': '石'},\n",
      " {'end_timestamps': [940, 1500, 2020, 2560],\n",
      "  'probabilities': [0.48731955885887146,\n",
      "                    0.39906826615333557,\n",
      "                    0.5395309329032898,\n",
      "                    0.5293596386909485],\n",
      "  'start_timestamps': [920, 1480, 2000, 2540],\n",
      "  'transcription': '石实实施'},\n",
      " {'end_timestamps': [1120],\n",
      "  'probabilities': [0.8874255418777466],\n",
      "  'start_timestamps': [1100],\n",
      "  'transcription': '四'},\n",
      " {'end_timestamps': [820, 1440, 2000, 2480],\n",
      "  'probabilities': [0.9924436211585999,\n",
      "                    0.988577127456665,\n",
      "                    0.9537660479545593,\n",
      "                    0.8817508220672607],\n",
      "  'start_timestamps': [800, 1420, 1980, 2460],\n",
      "  'transcription': '四四四四'},\n",
      " {'end_timestamps': [980],\n",
      "  'probabilities': [0.7496780753135681],\n",
      "  'start_timestamps': [960],\n",
      "  'transcription': '林'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "pprint(model.transcribe(dataset))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/09/2022 20:19:01 - INFO - huggingsound.speech_recognition.model - Loading training data...\n",
      "11/09/2022 20:19:01 - INFO - huggingsound.speech_recognition.model - Converting data format...\n",
      "11/09/2022 20:19:01 - INFO - huggingsound.speech_recognition.model - Preparing data input and labels...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/24 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "996ac2ada5e742d9bea0fb6cc6ae6550"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/09/2022 20:19:03 - INFO - huggingsound.speech_recognition.model - Starting fine-tuning process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/09/2022 20:19:03 - INFO - huggingsound.trainer - Getting dataset stats...\n",
      "11/09/2022 20:19:03 - INFO - huggingsound.trainer - Training dataset size: 24 samples, 0.017777777777777778 hours\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file config.json from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\config.json\n",
      "Model config Wav2Vec2Config {\n",
      "  \"_name_or_path\": \"wbbbbb/wav2vec2-large-chinese-zh-cn\",\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"adapter_kernel_size\": 3,\n",
      "  \"adapter_stride\": 2,\n",
      "  \"add_adapter\": false,\n",
      "  \"apply_spec_augment\": true,\n",
      "  \"architectures\": [\n",
      "    \"Wav2Vec2ForPreTraining\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_proj_size\": 256,\n",
      "  \"codevector_dim\": 768,\n",
      "  \"contrastive_logits_temperature\": 0.1,\n",
      "  \"conv_bias\": true,\n",
      "  \"conv_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512\n",
      "  ],\n",
      "  \"conv_kernel\": [\n",
      "    10,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    3,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"conv_stride\": [\n",
      "    5,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2,\n",
      "    2\n",
      "  ],\n",
      "  \"ctc_loss_reduction\": \"mean\",\n",
      "  \"ctc_zero_infinity\": false,\n",
      "  \"diversity_loss_weight\": 0.1,\n",
      "  \"do_stable_layer_norm\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feat_extract_activation\": \"gelu\",\n",
      "  \"feat_extract_dropout\": 0.0,\n",
      "  \"feat_extract_norm\": \"layer\",\n",
      "  \"feat_proj_dropout\": 0.0,\n",
      "  \"feat_quantizer_dropout\": 0.0,\n",
      "  \"final_dropout\": 0.0,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout\": 0.0,\n",
      "  \"hidden_size\": 1024,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 4096,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"layerdrop\": 0.1,\n",
      "  \"mask_channel_length\": 10,\n",
      "  \"mask_channel_min_space\": 1,\n",
      "  \"mask_channel_other\": 0.0,\n",
      "  \"mask_channel_prob\": 0.0,\n",
      "  \"mask_channel_selection\": \"static\",\n",
      "  \"mask_feature_length\": 10,\n",
      "  \"mask_feature_min_masks\": 0,\n",
      "  \"mask_feature_prob\": 0.0,\n",
      "  \"mask_time_length\": 10,\n",
      "  \"mask_time_min_masks\": 2,\n",
      "  \"mask_time_min_space\": 1,\n",
      "  \"mask_time_other\": 0.0,\n",
      "  \"mask_time_prob\": 0.05,\n",
      "  \"mask_time_selection\": \"static\",\n",
      "  \"model_type\": \"wav2vec2\",\n",
      "  \"num_adapter_layers\": 3,\n",
      "  \"num_attention_heads\": 16,\n",
      "  \"num_codevector_groups\": 2,\n",
      "  \"num_codevectors_per_group\": 320,\n",
      "  \"num_conv_pos_embedding_groups\": 16,\n",
      "  \"num_conv_pos_embeddings\": 128,\n",
      "  \"num_feat_extract_layers\": 7,\n",
      "  \"num_hidden_layers\": 24,\n",
      "  \"num_negatives\": 100,\n",
      "  \"output_hidden_size\": 1024,\n",
      "  \"pad_token_id\": 5168,\n",
      "  \"proj_codevector_dim\": 768,\n",
      "  \"tdnn_dilation\": [\n",
      "    1,\n",
      "    2,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"tdnn_dim\": [\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    512,\n",
      "    1500\n",
      "  ],\n",
      "  \"tdnn_kernel\": [\n",
      "    5,\n",
      "    3,\n",
      "    3,\n",
      "    1,\n",
      "    1\n",
      "  ],\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.24.0\",\n",
      "  \"use_weighted_layer_sum\": false,\n",
      "  \"vocab_size\": 5171,\n",
      "  \"xvector_output_dim\": 512\n",
      "}\n",
      "\n",
      "loading weights file pytorch_model.bin from cache at C:\\Users\\24103/.cache\\huggingface\\hub\\models--wbbbbb--wav2vec2-large-chinese-zh-cn\\snapshots\\369f73139f85a98570ff74e641dc93d421a3860e\\pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing Wav2Vec2ForCTC.\n",
      "\n",
      "All the weights of Wav2Vec2ForCTC were initialized from the model checkpoint at wbbbbb/wav2vec2-large-chinese-zh-cn.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Wav2Vec2ForCTC for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/09/2022 20:19:07 - INFO - huggingsound.trainer - Building trainer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\program\\python37\\lib\\site-packages\\transformers\\models\\wav2vec2\\modeling_wav2vec2.py:1638: FutureWarning: The method `freeze_feature_extractor` is deprecated and will be removed in Transformers v5.Please use the equivalent `freeze_feature_encoder` method instead.\n",
      "  FutureWarning,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/09/2022 20:19:08 - INFO - huggingsound.trainer - Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature extractor saved in train_output\\preprocessor_config.json\n",
      "tokenizer config file saved in train_output\\tokenizer_config.json\n",
      "Special tokens file saved in train_output\\special_tokens_map.json\n",
      "added tokens file saved in train_output\\added_tokens.json\n",
      "The following columns in the training set don't have a corresponding argument in `Wav2Vec2ForCTC.forward` and have been ignored: length. If length are not expected by `Wav2Vec2ForCTC.forward`,  you can safely ignore this message.\n",
      "***** Running training *****\n",
      "  Num examples = 24\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 9\n",
      "  Number of trainable parameters = 316528819\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 5.13 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_29724/900193710.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[1;32m----> 1\u001B[1;33m \u001B[0mmodel\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfinetune\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"train_output\"\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdataset\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32md:\\program\\python37\\lib\\site-packages\\huggingsound\\speech_recognition\\model.py\u001B[0m in \u001B[0;36mfinetune\u001B[1;34m(self, output_dir, train_data, eval_data, data_cache_dir, token_set, training_args, model_args, text_normalizer, num_workers)\u001B[0m\n\u001B[0;32m    359\u001B[0m         \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Starting fine-tuning process...\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    360\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 361\u001B[1;33m         \u001B[0mfinetune_ctc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mmodel_path\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0moutput_dir\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mprocessor\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtrain_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0meval_dataset\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdevice\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtraining_args\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmodel_args\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    362\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    363\u001B[0m         \u001B[0mlogger\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0minfo\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;34m\"Loading fine-tuned model...\"\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\program\\python37\\lib\\site-packages\\huggingsound\\trainer.py\u001B[0m in \u001B[0;36mfinetune_ctc\u001B[1;34m(model_name_or_path, output_dir, processor, train_dataset, eval_dataset, device, training_args, model_args)\u001B[0m\n\u001B[0;32m    669\u001B[0m         \u001B[0mprocessor\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_pretrained\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mhftraining_args\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moutput_dir\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    670\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m--> 671\u001B[1;33m     \u001B[0mtrain_result\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mcheckpoint\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m    672\u001B[0m     \u001B[0mtrainer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0msave_model\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m    673\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\program\\python37\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36mtrain\u001B[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[0;32m   1503\u001B[0m             \u001B[0mresume_from_checkpoint\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mresume_from_checkpoint\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1504\u001B[0m             \u001B[0mtrial\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtrial\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1505\u001B[1;33m             \u001B[0mignore_keys_for_eval\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mignore_keys_for_eval\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1506\u001B[0m         )\n\u001B[0;32m   1507\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\program\\python37\\lib\\site-packages\\transformers\\trainer.py\u001B[0m in \u001B[0;36m_inner_training_loop\u001B[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001B[0m\n\u001B[0;32m   1814\u001B[0m                         \u001B[0moptimizer_was_run\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mscale_before\u001B[0m \u001B[1;33m<=\u001B[0m \u001B[0mscale_after\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1815\u001B[0m                     \u001B[1;32melse\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 1816\u001B[1;33m                         \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0moptimizer\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mstep\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m   1817\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   1818\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0moptimizer_was_run\u001B[0m \u001B[1;32mand\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mdeepspeed\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\program\\python37\\lib\\site-packages\\torch\\optim\\lr_scheduler.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     63\u001B[0m                 \u001B[0minstance\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_step_count\u001B[0m \u001B[1;33m+=\u001B[0m \u001B[1;36m1\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     64\u001B[0m                 \u001B[0mwrapped\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__get__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0minstance\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 65\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mwrapped\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     66\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     67\u001B[0m             \u001B[1;31m# Note that the returned function here is no longer a bound method,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\program\\python37\\lib\\site-packages\\torch\\optim\\optimizer.py\u001B[0m in \u001B[0;36mwrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     86\u001B[0m                 \u001B[0mprofile_name\u001B[0m \u001B[1;33m=\u001B[0m \u001B[1;34m\"Optimizer.step#{}.step\"\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mobj\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__name__\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     87\u001B[0m                 \u001B[1;32mwith\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mautograd\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mprofiler\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mrecord_function\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mprofile_name\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 88\u001B[1;33m                     \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     89\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     90\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\program\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001B[0m in \u001B[0;36mdecorate_context\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     26\u001B[0m         \u001B[1;32mdef\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     27\u001B[0m             \u001B[1;32mwith\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__class__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 28\u001B[1;33m                 \u001B[1;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m*\u001B[0m\u001B[0margs\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     29\u001B[0m         \u001B[1;32mreturn\u001B[0m \u001B[0mcast\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mF\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mdecorate_context\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     30\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32md:\\program\\python37\\lib\\site-packages\\torch\\optim\\adamw.py\u001B[0m in \u001B[0;36mstep\u001B[1;34m(self, closure)\u001B[0m\n\u001B[0;32m     92\u001B[0m                     \u001B[0mstate\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'exp_avg'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmemory_format\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreserve_format\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     93\u001B[0m                     \u001B[1;31m# Exponential moving average of squared gradient values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 94\u001B[1;33m                     \u001B[0mstate\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;34m'exp_avg_sq'\u001B[0m\u001B[1;33m]\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mzeros_like\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mp\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mmemory_format\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mtorch\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mpreserve_format\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m     95\u001B[0m                     \u001B[1;32mif\u001B[0m \u001B[0mamsgrad\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     96\u001B[0m                         \u001B[1;31m# Maintains max of all exp. moving avg. of sq. grad. values\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 16.00 MiB (GPU 0; 6.00 GiB total capacity; 5.13 GiB already allocated; 0 bytes free; 5.29 GiB reserved in total by PyTorch)"
     ]
    }
   ],
   "source": [
    "model.finetune(\"train_output\", dataset)"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
