{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec - English Single"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import librosa\n",
    "from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2FeatureExtractor\n",
    "import numpy as np\n",
    "model_name = \"skpawar1305/wav2vec2-base-finetuned-digits\"\n",
    "model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)\n",
    "feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "PATH = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\pycharm\\digital_signal\\speech_signal_processing\\ThirdPartyRepos\\Wav2Vec/12345_68772.wav\n",
      "True\n",
      "db = -21.15406995820191\n"
     ]
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "import sounddevice as sd\n",
    "\n",
    "test_file = PATH+\"/12345_68772.wav\"\n",
    "print(test_file)\n",
    "print(os.path.exists(test_file))\n",
    "def get_nparray(audiosegment: AudioSegment):\n",
    "    samples = audiosegment.get_array_of_samples()\n",
    "    samples_float = librosa.util.buf_to_float(samples,n_bytes=4, dtype=np.float32)\n",
    "    if audiosegment.channels==2:\n",
    "        sample_left= np.copy(samples_float[::2])\n",
    "        sample_right= np.copy(samples_float[1::2])\n",
    "        sample_all = np.array([sample_left,sample_right])\n",
    "    else:\n",
    "        sample_all = samples_float\n",
    "\n",
    "    return sample_all\n",
    "\n",
    "\n",
    "data = AudioSegment.from_wav(test_file)\n",
    "print(f\"db = {data.dBFS}\")\n",
    "sd.play(get_nparray(data), samplerate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [00:06<00:00,  1.43it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "chunks = split_on_silence(data, min_silence_len=20, keep_silence=40, silence_thresh=-25)\n",
    "print(len(chunks))\n",
    "for _c in tqdm(chunks):\n",
    "    sd.play(get_nparray(_c), samplerate=16000)\n",
    "    sd.wait()\n",
    "\n",
    "# sd.play(get_nparray(data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "one\n",
      "_unknown_\n",
      "_unknown_\n",
      "two\n",
      "_unknown_\n",
      "_unknown_\n",
      "three\n",
      "four\n",
      "five\n"
     ]
    }
   ],
   "source": [
    "def inference(chunk):\n",
    "    inputs = feature_extractor(chunk, sampling_rate=16000, padding=True, return_tensors=\"pt\")\n",
    "\n",
    "    logits = model(**inputs).logits\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    labels = [model.config.id2label[_id] for _id in predicted_ids.tolist()]\n",
    "    if not len(labels):\n",
    "        print(\"dead\")\n",
    "    return labels[0]\n",
    "\n",
    "for _c in chunks:\n",
    "    print(inference(get_nparray(_c)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wav2Vec - Chinese Sequence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingsound import SpeechRecognitionModel\n",
    "from pprint import pprint\n",
    "import os\n",
    "\n",
    "model = SpeechRecognitionModel(\"wbbbbb/wav2vec2-large-chinese-zh-cn\", device=\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = './zh_seq'\n",
    "dataset = [os.path.join(dataset, f) for f in os.listdir(dataset)]\n",
    "eval_files = dataset.copy()\n",
    "tmp = []\n",
    "for d in dataset:\n",
    "    label = os.path.splitext(os.path.split(d)[1])[0].split('_')[0]\n",
    "    tmp.append({\"path\": d, \"transcription\": label})\n",
    "dataset = tmp\n",
    "del tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(model.transcribe(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.finetune(\"train_output\", dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
