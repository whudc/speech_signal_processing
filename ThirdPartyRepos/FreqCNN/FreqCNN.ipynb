{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Load Pretrained"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "total_data = np.load(\"data.npy\")\n",
    "total_label = np.load(\"label.npy\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(total_data, total_label, test_size=0.33, random_state=42)\n",
    "X_train_len, X_test_len = len(X_train), len(X_test)\n",
    "ratio = 1\n",
    "X_train = X_train[:int(X_train_len * ratio)]\n",
    "X_test = X_test[:int(X_test_len * ratio)]\n",
    "y_train = y_train[:int(X_train_len * ratio)]\n",
    "y_test = y_test[:int(X_test_len * ratio)]\n",
    "X_train_len, X_test_len = len(X_train), len(X_test)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CNN\n",
    "## Model"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[  1.1844,  -8.3257,  -0.5414, -55.2046,   7.1606, -33.5012,  31.8390,\n         -18.3668, -35.7643,  10.0661]], device='cuda:0',\n       grad_fn=<AddmmBackward>)"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.channels = channels\n",
    "\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = F.relu(self.conv1(x))\n",
    "        y = self.conv2(y)\n",
    "        return F.relu(x + y)\n",
    "\n",
    "#实现一个Bottleneck的类，初始化需要输入通道数与GrowthRate这两个参数\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self,nChannels,growthRate):\n",
    "        super(Bottleneck,self).__init__()\n",
    "    #通常1x1卷积的通道数为GrowthRate的4倍\n",
    "        interChannels=4*growthRate\n",
    "        self.bn1=nn.BatchNorm2d(nChannels)\n",
    "        self.conv1=nn.Conv2d(nChannels,interChannels,kernel_size=1,bias=False)\n",
    "        self.bn2=nn.BatchNorm2d(interChannels)\n",
    "        self.conv2=nn.Conv2d(interChannels,growthRate,kernel_size=3,padding=1,bias=False)\n",
    "\n",
    "    def forward(self,x):\n",
    "        out=self.conv1(F.relu(self.bn1(x)))\n",
    "        out=self.conv2(F.relu(self.bn2(out)))\n",
    "        #将输入x同计算的结果out进行通道拼接\n",
    "        out=torch.cat((x,out),1)\n",
    "        return out\n",
    "\n",
    "class Denseblock(nn.Module):\n",
    "    def __init__(self,nChannels,growthRate,nDenseBlocks):\n",
    "        super(Denseblock,self).__init__()\n",
    "        layers=[]\n",
    "        #将每一个Bottleneck利用nn.Sequential()整合起来，输入通道数需要线性增长\n",
    "        for i in range(int(nDenseBlocks)):\n",
    "            layers.append(Bottleneck(nChannels,growthRate))\n",
    "            nChannels+=growthRate\n",
    "        self.denseblock=nn.Sequential(*layers)\n",
    "    def forward(self,x):\n",
    "        return self.denseblock(x)\n",
    "\n",
    "\n",
    "class CnnModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CnnModel, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=5),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, kernel_size=5),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # self.conv3 = nn.Sequential(\n",
    "        #     nn.Conv2d(32, 64, kernel_size=5),\n",
    "        #     nn.BatchNorm2d(64),\n",
    "        #     nn.ReLU()\n",
    "        # )\n",
    "        self.mp = nn.MaxPool2d(4)\n",
    "\n",
    "        self.rblock1 = ResidualBlock(16)\n",
    "        self.rblock2 = ResidualBlock(32)\n",
    "        # self.rblock3 = ResidualBlock(64)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "        self._set_init(self.conv1)\n",
    "        self._set_init(self.conv2)\n",
    "        # self._set_init(self.conv3)\n",
    "        self._set_init(self.fc)\n",
    "\n",
    "    def _set_init(self, sequence: nn.Sequential):  # 参数初始化\n",
    "        for name, param in sequence.named_parameters():\n",
    "            if 'weight' in name:\n",
    "                init.normal_(param, 0, 1.0)\n",
    "            if 'bias' in name:\n",
    "                init.constant_(param, 0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Flatten data from (n,1,28,28) to (n,784)\n",
    "        in_size = x.size(0)\n",
    "        x = self.mp(self.conv1(x))\n",
    "        x = self.rblock1(x)\n",
    "        x = self.mp(self.conv2(x))\n",
    "        x = self.rblock2(x)\n",
    "        # x = self.mp(self.conv3(x))\n",
    "        # x = self.rblock3(x)\n",
    "        # x = self.mp(x)\n",
    "        # x = self.mp(F.relu(self.conv3(x)))\n",
    "        # x = self.rblock3(x)\n",
    "        x = x.view(in_size, -1)  # flatten\n",
    "        #         print(x.size(1))\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "_model = CnnModel().float().cuda()\n",
    "\n",
    "_test_input = torch.rand((1, 1, 56, 56)).cuda()\n",
    "_model(_test_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[ 0.1628, -0.0028, -0.2856,  0.0635, -0.0646,  0.3127, -0.2245,  0.5216,\n          0.2237, -0.2657]], device='cuda:0', grad_fn=<AddmmBackward>)"
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "class _DenseLayer(nn.Sequential):\n",
    "    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n",
    "        super(_DenseLayer, self).__init__()\n",
    "        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n",
    "        self.add_module('relu1', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n",
    "                                           growth_rate, kernel_size=1, stride=1, bias=False)),\n",
    "        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n",
    "        self.add_module('relu2', nn.ReLU(inplace=True)),\n",
    "        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n",
    "                                           kernel_size=3, stride=1, padding=1, bias=False)),\n",
    "        self.drop_rate = drop_rate\n",
    "\n",
    "    def forward(self, x):\n",
    "        new_features = super(_DenseLayer, self).forward(x)\n",
    "        if self.drop_rate > 0:\n",
    "            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n",
    "        return torch.cat([x, new_features], 1)\n",
    "\n",
    "\n",
    "class _DenseBlock(nn.Sequential):\n",
    "    def __init__(self, num_layers, num_input_features, bn_size, growth_rate, drop_rate):\n",
    "        super(_DenseBlock, self).__init__()\n",
    "        for i in range(num_layers):\n",
    "            layer = _DenseLayer(num_input_features + i * growth_rate, growth_rate, bn_size, drop_rate)\n",
    "            self.add_module('denselayer%d' % (i + 1), layer)\n",
    "\n",
    "\n",
    "class _Transition(nn.Sequential):\n",
    "    def __init__(self, num_input_features, num_output_features):\n",
    "        super(_Transition, self).__init__()\n",
    "        self.add_module('norm', nn.BatchNorm2d(num_input_features))\n",
    "        self.add_module('relu', nn.ReLU(inplace=True))\n",
    "        self.add_module('conv', nn.Conv2d(num_input_features, num_output_features,\n",
    "                                          kernel_size=1, stride=1, bias=False))\n",
    "        self.add_module('pool', nn.AvgPool2d(kernel_size=2, stride=2))\n",
    "\n",
    "\n",
    "class DenseNet(nn.Module):\n",
    "    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n",
    "                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n",
    "\n",
    "        super(DenseNet, self).__init__()\n",
    "\n",
    "        # First convolution\n",
    "        self.features = nn.Sequential(OrderedDict([\n",
    "            ('conv0', nn.Conv2d(1, num_init_features, kernel_size=3, bias=False)),\n",
    "            ('norm0', nn.BatchNorm2d(num_init_features)),\n",
    "            ('relu0', nn.ReLU(inplace=True)),\n",
    "            ('pool0', nn.MaxPool2d(kernel_size=2)),\n",
    "        ]))\n",
    "\n",
    "        # Each denseblock\n",
    "        num_features = num_init_features\n",
    "        for i, num_layers in enumerate(block_config):\n",
    "            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n",
    "                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n",
    "            self.features.add_module('denseblock%d' % (i + 1), block)\n",
    "            num_features = num_features + num_layers * growth_rate\n",
    "            if i != len(block_config) - 1:\n",
    "                trans = _Transition(num_input_features=num_features, num_output_features=num_features // 2)\n",
    "                self.features.add_module('transition%d' % (i + 1), trans)\n",
    "                num_features = num_features // 2\n",
    "\n",
    "        # Final batch norm\n",
    "        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n",
    "\n",
    "        # Linear layer\n",
    "        num_features = 1150\n",
    "        self.classifier = nn.Linear(num_features, num_classes)\n",
    "\n",
    "        # Official init from torch repo.\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight.data)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.features(x)\n",
    "        out = F.relu(features, inplace=True)\n",
    "        out = F.avg_pool2d(out, kernel_size=2, stride=1).view(features.size(0), -1)\n",
    "        # print(out.shape)\n",
    "        out = self.classifier(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "net = DenseNet(num_init_features=1, growth_rate=4, block_config=(6, 12, 4), num_classes=10)\n",
    "# print(net)\n",
    "_model = net.float().cuda()\n",
    "\n",
    "_test_input = torch.rand((1, 1, 56, 56)).cuda()\n",
    "_model(_test_input)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Prepare Train Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [],
   "source": [
    "\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch.utils.data as Data\n",
    "\n",
    "writer = SummaryWriter('runs/train')\n",
    "\n",
    "\n",
    "# class FreqData(Dataset):\n",
    "#     def __len__(self):\n",
    "#         return len(X_train)\n",
    "#\n",
    "#     def __getitem__(self, index):\n",
    "#         return X_train[index, :, :], y_train[index]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "outputs": [],
   "source": [
    "\n",
    "bs = 100\n",
    "batch = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "outputs": [],
   "source": [
    "\n",
    "_loss_fn = nn.CrossEntropyLoss()\n",
    "# _optimizer = torch.optim.SGD(_model.parameters(), lr=1e-4)\n",
    "_optimizer = torch.optim.AdamW(_model.parameters(), lr=1e-4)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [],
   "source": [
    "\n",
    "def np2ts(a, l):\n",
    "    a = torch.Tensor(a)\n",
    "    a = a.unsqueeze(1)  # used for CNN data\n",
    "    l = torch.Tensor(l).long()\n",
    "    return a, l\n",
    "\n",
    "train_dataset = Data.TensorDataset(*np2ts(X_train, y_train))\n",
    "train_loader = Data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=bs,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "test_dataset = Data.TensorDataset(*np2ts(X_test, y_test))\n",
    "test_loader = Data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=bs,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "def to_cuda(*ts: torch.Tensor):\n",
    "    return [_ts.cuda() for _ts in ts]\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Evaluation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [],
   "source": [
    "\n",
    "def eval():\n",
    "    accuracy = []\n",
    "    _model.eval()\n",
    "    for stp, (test_x, test_y) in enumerate(test_loader):\n",
    "        test_x, test_y = to_cuda(test_x, test_y)\n",
    "        test_output = _model(test_x)\n",
    "        _, pred_y = torch.max(test_output, 1)\n",
    "        accuracy.append(torch.sum(pred_y == test_y).cpu().item() / len(test_y))\n",
    "    score = np.mean(accuracy)\n",
    "    return score\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "## Train"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [],
   "source": [
    "max_val_score = 0\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8450, 0.22914332151412964 <-> 0.8659493670886076\n",
      "best model at 8450\n",
      "8500, 0.22425487637519836 <-> 0.8648101265822786\n",
      "8550, 0.11168172210454941 <-> 0.8640506329113925\n",
      "8600, 0.17536665499210358 <-> 0.8621518987341772\n",
      "8650, 0.22484250366687775 <-> 0.8659493670886077\n",
      "best model at 8650\n",
      "8700, 0.2538182735443115 <-> 0.8675949367088607\n",
      "best model at 8700\n",
      "8750, 0.15746936202049255 <-> 0.8664556962025317\n",
      "8800, 0.3418591320514679 <-> 0.8639240506329116\n"
     ]
    }
   ],
   "source": [
    "\n",
    "is_break = False\n",
    "while True:\n",
    "    if is_break:\n",
    "        break\n",
    "    for b, (_X, _y) in enumerate(train_loader):\n",
    "        _model.train()\n",
    "        _X = _X.float()  #.cuda()\n",
    "        _y = _y.long()  #.cuda()\n",
    "        _X, _y = to_cuda(_X, _y)\n",
    "        pred = _model(_X)\n",
    "        loss = _loss_fn(pred, _y)\n",
    "\n",
    "        # BP\n",
    "        _optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        _optimizer.step()\n",
    "        batch += 1\n",
    "\n",
    "        if batch % 50 == 0:\n",
    "            _model.eval()\n",
    "            eval_score = eval()\n",
    "            print(f\"{batch}, {loss} <-> {eval_score}\")\n",
    "            writer.add_scalars('CNN Training Loss',\n",
    "                               {'Train': loss, 'Val': eval_score},\n",
    "                               batch)\n",
    "            if eval_score > max_val_score:\n",
    "                torch.save(_model.state_dict(), f\"cnn-model-best.pth\")\n",
    "                print(f\"best model at {batch}\")\n",
    "                max_val_score = eval_score\n",
    "        if batch % 400 == 0:\n",
    "            is_break = True\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'float'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp/ipykernel_29520/939056809.py\u001B[0m in \u001B[0;36m<module>\u001B[1;34m\u001B[0m\n\u001B[0;32m      5\u001B[0m     \u001B[1;32mfor\u001B[0m \u001B[0mb\u001B[0m\u001B[1;33m,\u001B[0m \u001B[1;33m(\u001B[0m\u001B[0m_X\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_y\u001B[0m\u001B[1;33m)\u001B[0m \u001B[1;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_mfcc_data\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      6\u001B[0m         \u001B[0m_model\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtrain\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m----> 7\u001B[1;33m         \u001B[0m_X\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_X\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfloat\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m#.cuda()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m\u001B[0;32m      8\u001B[0m         \u001B[0m_y\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0m_y\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mlong\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m  \u001B[1;31m#.cuda()\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m      9\u001B[0m         \u001B[0m_X\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_y\u001B[0m \u001B[1;33m=\u001B[0m \u001B[0mto_cuda\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0m_X\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0m_y\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'float'"
     ]
    }
   ],
   "source": [
    "torch.save(_model.state_dict(), f\"cnn-model-{batch}.pth\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
